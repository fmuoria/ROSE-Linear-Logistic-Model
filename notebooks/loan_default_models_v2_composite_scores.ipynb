{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# ROSE Women's Foundation - Improved Loan Default Prediction\n## Using Composite Risk Scores for Enhanced Performance\n\nThis notebook implements an improved predictive model leveraging the 4 composite scores from EDA:\n- **Financial Resilience Score** (0-100)\n- **Business Quality Score** (0-100)\n- **Stability Score** (0-100)\n- **Expense Management Score** (0-100)\n\n### Baseline vs Target Performance\n| Metric | Baseline (v1) | Target (v2) |\n|--------|---------------|-------------|\n| KS Statistic | 0.21 | \u22650.28 |\n| ROC-AUC | 0.60 | \u22650.68 |\n\n### Approach\n- Test 3 feature set variations (A, B, C)\n- Train 5 algorithms per feature set (15 total models)\n- Compare performance to identify best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nimport joblib\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Evaluation\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, precision_recall_curve,\n    confusion_matrix, classification_report\n)\nfrom scipy import stats\n\n# Interpretability\nimport shap\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nsns.set_palette(\"colorblind\")\npd.set_option(\"display.max_columns\", None)\n\n# Set random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Create output directory\nos.makedirs(\"../models/v2\", exist_ok=True)\n\nprint(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n# Section 1: Data Loading & Composite Score Generation\n\nLoad raw data and generate the 4 composite risk scores from EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\nDATA_PATH = \"../Github Original Data.csv\"\ndf = pd.read_csv(DATA_PATH, encoding=\"latin-1\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nTarget variable 'Defaulted' distribution:\")\nprint(df[\"Defaulted\"].value_counts())\nprint(f\"\\nDefault rate: {df['Defaulted'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-header",
   "metadata": {},
   "source": [
    "### Generate Intermediate Features (from EDA)\n\nThese intermediate features are needed for composite score calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate intermediate features needed for composite scores\n\n# Affordability Business\nif \"Affordability\" in df.columns:\n    df[\"Affordability_Business\"] = df[\"Affordability\"].fillna(\"Unknown\")\nelse:\n    df[\"Affordability_Business\"] = \"Unknown\"\n\n# Affordability HH (Household)\nif \"Affordability (HH)\" in df.columns:\n    df[\"Affordability_HH\"] = df[\"Affordability (HH)\"].fillna(\"Unknown\")\nelse:\n    df[\"Affordability_HH\"] = df.get(\"Affordability_Business\", \"Unknown\")\n\n# Extra Income Brackets\nif \"Extra Income Brackets\" in df.columns:\n    df[\"Extra_Income_Brackets\"] = df[\"Extra Income Brackets\"].fillna(\"No Extra Income\")\nelse:\n    extra_income = pd.to_numeric(df.get(\"Extra Income\", 0), errors=\"coerce\").fillna(0)\n    df[\"Extra_Income_Brackets\"] = np.where(\n        extra_income == 0, \"No Extra Income\",\n        np.where(extra_income <= 16000, \"Low Extra Income\", \"Moderate to High Extra Income\")\n    )\n\n# Regular Income Brackets\nif \"Regular Income Brackets\" in df.columns:\n    df[\"Regular_Income_Brackets\"] = df[\"Regular Income Brackets\"].fillna(\"No Regular Income\")\nelse:\n    regular_income = pd.to_numeric(df.get(\"Regular monthly income\", 0), errors=\"coerce\").fillna(0)\n    df[\"Regular_Income_Brackets\"] = np.where(\n        regular_income == 0, \"No Regular Income\",\n        np.where(regular_income <= 7800, \"Low Regular Income\", \"Moderate/High Regular Income\")\n    )\n\n# Income Diversity\nif \"Logic on Income\" in df.columns:\n    df[\"Income_Diversity\"] = df[\"Logic on Income\"].fillna(\"Unknown\")\nelse:\n    df[\"Income_Diversity\"] = \"Income Only\"\n\n# Expense Ratio\nif \"Expense Relative to Income\" in df.columns:\n    df[\"Expense_Ratio\"] = df[\"Expense Relative to Income\"].fillna(\"Unknown\")\nelse:\n    df[\"Expense_Ratio\"] = \"Unknown\"\n\n# Utility Category\nif \"Categorizing Utility Expenses\" in df.columns:\n    df[\"Utility_Category\"] = df[\"Categorizing Utility Expenses\"].fillna(\"Unknown\")\nelse:\n    utility = pd.to_numeric(df.get(\"Utility Expenses\", 0), errors=\"coerce\").fillna(0)\n    df[\"Utility_Category\"] = np.where(\n        utility == 0, \"No Utility Expenses\",\n        np.where(utility > 12000, \"High Utility Expenses\", \"Low Utility Expenses\")\n    )\n\n# Rent Category\nif \"Categorize Rent Payment\" in df.columns:\n    df[\"Rent_Category\"] = df[\"Categorize Rent Payment\"].fillna(\"Unknown\")\nelse:\n    rent = pd.to_numeric(df.get(\"Rent per month\", 0), errors=\"coerce\").fillna(0)\n    df[\"Rent_Category\"] = np.where(\n        rent == 0, \"No Rent Paid\",\n        np.where(rent > 5000, \"High Rent\", \"Low Rent\")\n    )\n\n# School Fees Category\nif \"School Fees Categorical\" in df.columns:\n    df[\"SchoolFees_Category\"] = df[\"School Fees Categorical\"].fillna(\"Unknown\")\nelse:\n    school_fees = pd.to_numeric(df.get(\"School Fees\", 0), errors=\"coerce\").fillna(0)\n    df[\"SchoolFees_Category\"] = np.where(\n        school_fees == 0, \"No School Fees\",\n        np.where(school_fees > 50000, \"High School Fees\", \"Low School Fees\")\n    )\n\n# Savings Category\nif \"Savings Categorical\" in df.columns:\n    df[\"Savings_Category\"] = df[\"Savings Categorical\"].fillna(\"Unknown\")\nelse:\n    savings = pd.to_numeric(df.get(\"Average monthly savings\", 0), errors=\"coerce\").fillna(0)\n    df[\"Savings_Category\"] = np.where(\n        savings == 0, \"No Savings\",\n        np.where(savings > 2350, \"High Savings\", \"Low Savings\")\n    )\n\nprint(\"Intermediate features generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-functions-header",
   "metadata": {},
   "source": [
    "### Composite Score Calculation Functions\n\nThese functions are from the EDA notebook (Section 9.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_financial_resilience(row):\n    \"\"\"\n    Financial Resilience Score (0-100)\n    - Extra Income Level: 35% weight\n    - Expense-to-Income Ratio: 30% weight\n    - Income Diversity: 20% weight\n    - Savings Level: 15% weight\n    \"\"\"\n    score = 0\n    \n    extra_income = str(row.get(\"Extra_Income_Brackets\", \"\")).lower()\n    if \"moderate\" in extra_income or \"high\" in extra_income:\n        score += 35 * 1.0\n    elif \"low\" in extra_income and \"no\" not in extra_income:\n        score += 35 * 0.3\n    else:\n        score += 35 * 0.6\n    \n    expense_ratio = str(row.get(\"Expense_Ratio\", \"\")).lower()\n    if \"1/3\" in expense_ratio:\n        score += 30 * 1.0\n    elif \"half\" in expense_ratio:\n        score += 30 * 0.7\n    elif \"2/3\" in expense_ratio and \"more\" not in expense_ratio:\n        score += 30 * 0.4\n    else:\n        score += 30 * 0.5\n    \n    income_div = str(row.get(\"Income_Diversity\", \"\")).lower()\n    if \"full\" in income_div:\n        score += 20 * 1.0\n    elif \"regular\" in income_div:\n        score += 20 * 0.7\n    elif \"extra\" in income_div:\n        score += 20 * 0.5\n    else:\n        score += 20 * 0.6\n    \n    savings = str(row.get(\"Savings_Category\", \"\")).lower()\n    if \"high\" in savings:\n        score += 15 * 1.0\n    elif \"low\" in savings and \"no\" not in savings:\n        score += 15 * 0.8\n    else:\n        score += 15 * 0.85\n    \n    return score\n\n\ndef calculate_business_quality(row):\n    \"\"\"\n    Business Quality Score (0-100)\n    - Rent Payment Level: 45% weight\n    - Utility Expenses: 30% weight\n    - Business Affordability: 25% weight\n    \"\"\"\n    score = 0\n    \n    rent = str(row.get(\"Rent_Category\", \"\")).lower()\n    if \"high\" in rent:\n        score += 45 * 1.0\n    elif \"low\" in rent and \"no\" not in rent:\n        score += 45 * 0.5\n    else:\n        score += 45 * 0.6\n    \n    utility = str(row.get(\"Utility_Category\", \"\")).lower()\n    if \"high\" in utility:\n        score += 30 * 1.0\n    elif \"low\" in utility and \"no\" not in utility:\n        score += 30 * 0.5\n    else:\n        score += 30 * 0.7\n    \n    afford = str(row.get(\"Affordability_Business\", \"\")).lower()\n    if \"profitable\" in afford:\n        score += 25 * 1.0\n    else:\n        score += 25 * 0.5\n    \n    return score\n\n\ndef calculate_stability(row):\n    \"\"\"\n    Stability Score (0-100)\n    - School Fees Commitment: 40% weight\n    - Regular Income Presence: 30% weight\n    - Multiple Income Streams: 30% weight\n    \"\"\"\n    score = 0\n    \n    school = str(row.get(\"SchoolFees_Category\", \"\")).lower()\n    if \"high\" in school:\n        score += 40 * 1.0\n    elif \"low\" in school and \"no\" not in school:\n        score += 40 * 0.5\n    else:\n        score += 40 * 0.9\n    \n    regular = str(row.get(\"Regular_Income_Brackets\", \"\")).lower()\n    if \"moderate\" in regular or \"high\" in regular:\n        score += 30 * 1.0\n    elif \"low\" in regular and \"no\" not in regular:\n        score += 30 * 1.1\n    else:\n        score += 30 * 0.85\n    \n    income_div = str(row.get(\"Income_Diversity\", \"\")).lower()\n    if \"full\" in income_div:\n        score += 30 * 1.0\n    elif \"regular\" in income_div:\n        score += 30 * 0.8\n    elif \"extra\" in income_div:\n        score += 30 * 0.6\n    else:\n        score += 30 * 0.7\n    \n    return min(score, 100)\n\n\ndef calculate_expense_management(row):\n    \"\"\"\n    Expense Management Score (0-100)\n    - Expense Relative to Income: 50% weight\n    - Affordability HH: 35% weight\n    - Utility Expenses: 15% weight\n    \"\"\"\n    score = 0\n    \n    expense_ratio = str(row.get(\"Expense_Ratio\", \"\")).lower()\n    if \"1/3\" in expense_ratio:\n        score += 50 * 1.0\n    elif \"half\" in expense_ratio:\n        score += 50 * 0.7\n    elif \"2/3\" in expense_ratio and \"more\" not in expense_ratio:\n        score += 50 * 0.4\n    else:\n        score += 50 * 0.5\n    \n    afford = str(row.get(\"Affordability_HH\", \"\")).lower()\n    if \"profitable\" in afford:\n        score += 35 * 1.0\n    else:\n        score += 35 * 0.5\n    \n    utility = str(row.get(\"Utility_Category\", \"\")).lower()\n    if \"high\" in utility:\n        score += 15 * 1.0\n    elif \"low\" in utility and \"no\" not in utility:\n        score += 15 * 0.5\n    else:\n        score += 15 * 0.7\n\n    return score\n\nprint(\"Composite score functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-scores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 4 composite scores\ndf[\"Financial_Resilience_Score\"] = df.apply(calculate_financial_resilience, axis=1)\ndf[\"Business_Quality_Score\"] = df.apply(calculate_business_quality, axis=1)\ndf[\"Stability_Score\"] = df.apply(calculate_stability, axis=1)\ndf[\"Expense_Management_Score\"] = df.apply(calculate_expense_management, axis=1)\n\nprint(\"Composite Scores Generated:\")\nprint(\"=\" * 50)\ncomposite_cols = [\"Financial_Resilience_Score\", \"Business_Quality_Score\", \n                  \"Stability_Score\", \"Expense_Management_Score\"]\n\nfor col in composite_cols:\n    print(f\"\\n{col}:\")\n    print(f\"  Mean: {df[col].mean():.2f}\")\n    print(f\"  Std:  {df[col].std():.2f}\")\n    print(f\"  Min:  {df[col].min():.2f}\")\n    print(f\"  Max:  {df[col].max():.2f}\")\n    print(f\"  Corr with Default: {df[col].corr(df['Defaulted']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-scores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize composite score distributions by default status\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor idx, col in enumerate(composite_cols):\n    defaulted = df[df[\"Defaulted\"] == 1][col]\n    paid = df[df[\"Defaulted\"] == 0][col]\n    \n    axes[idx].hist(paid, bins=20, alpha=0.6, label=\"Paid\", color=\"green\")\n    axes[idx].hist(defaulted, bins=20, alpha=0.6, label=\"Defaulted\", color=\"red\")\n    axes[idx].set_xlabel(col.replace(\"_\", \" \"))\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].set_title(f'{col.replace(\"_\", \" \")}\\nDefault vs Paid Distribution')\n    axes[idx].legend()\n    axes[idx].axvline(paid.mean(), color=\"green\", linestyle=\"--\", linewidth=2)\n    axes[idx].axvline(defaulted.mean(), color=\"red\", linestyle=\"--\", linewidth=2)\n\nplt.tight_layout()\nplt.savefig(\"../models/v2/composite_score_distributions.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n# Section 2: Feature Set Definitions\n\nDefine 3 feature set variations to test:\n- **Model A**: Composite Scores Only (4 features)\n- **Model B**: Composite + Key Categoricals (8 features)\n- **Model C**: Composite + Extended Categoricals, NO Prior Loan (10 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-sets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3 feature set variations\n\n# Model A: Composite Scores Only (4 features)\nFEATURES_A = [\n    \"Financial_Resilience_Score\",\n    \"Business_Quality_Score\",\n    \"Stability_Score\",\n    \"Expense_Management_Score\"\n]\n\n# Model B: Composite + Key Categoricals (8 features)\nFEATURES_B = [\n    # Composite Scores\n    \"Financial_Resilience_Score\",\n    \"Business_Quality_Score\",\n    \"Stability_Score\",\n    \"Expense_Management_Score\",\n    # Key Categorical Features\n    \"Age Group\",\n    \"Education\",\n    \"CRB Class\",\n    \"Living\"\n]\n\n# Model C: Composite + Extended Categoricals, NO Prior Loan (10 features)\nFEATURES_C = [\n    # Composite Scores\n    \"Financial_Resilience_Score\",\n    \"Business_Quality_Score\",\n    \"Stability_Score\",\n    \"Expense_Management_Score\",\n    # Categorical Features\n    \"Age Group\",\n    \"Education\",\n    \"CRB Class\",\n    \"Living\",\n    \"Logic on Income\",\n    \"Marital status\"\n    # NOTE: Explicitly EXCLUDING 'Loan Access' (Prior Loan) to test if removal helps\n]\n\nTARGET = \"Defaulted\"\n\n# Verify all features exist\nfeature_sets = {\"A\": FEATURES_A, \"B\": FEATURES_B, \"C\": FEATURES_C}\n\nfor name, features in feature_sets.items():\n    available = [f for f in features if f in df.columns]\n    missing = [f for f in features if f not in df.columns]\n    print(f\"Feature Set {name}: {len(available)}/{len(features)} available\")\n    if missing:\n        print(f\"  Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n# Section 3: Data Preprocessing\n\n- Handle missing values\n- Encode categorical features\n- Split data (70/15/15)\n- Apply SMOTE for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, features, target, test_size=0.15, val_size=0.15):\n    \"\"\"\n    Prepare data for a specific feature set.\n    Returns: X_train, X_val, X_test, y_train, y_val, y_test, encoders, scaler\n    \"\"\"\n    # Create working dataframe\n    available_features = [f for f in features if f in df.columns]\n    df_work = df[available_features + [target]].copy()\n    \n    # Handle missing values\n    for col in available_features:\n        if df_work[col].dtype == \"object\":\n            mode_val = df_work[col].mode()[0] if len(df_work[col].mode()) > 0 else \"Unknown\"\n            df_work[col] = df_work[col].fillna(mode_val)\n        else:\n            df_work[col] = df_work[col].fillna(df_work[col].median())\n    \n    # Encode categorical features\n    encoders = {}\n    for col in available_features:\n        if df_work[col].dtype == \"object\":\n            le = LabelEncoder()\n            df_work[col] = le.fit_transform(df_work[col].astype(str))\n            encoders[col] = le\n    \n    # Prepare X and y\n    X = df_work[available_features]\n    y = df_work[target]\n    \n    # Train/Val/Test split (70/15/15)\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n    )\n    \n    # Validation split from remaining data\n    val_ratio = val_size / (1 - test_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n    )\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return {\n        \"X_train\": X_train_scaled,\n        \"X_val\": X_val_scaled,\n        \"X_test\": X_test_scaled,\n        \"y_train\": y_train.values,\n        \"y_val\": y_val.values,\n        \"y_test\": y_test.values,\n        \"encoders\": encoders,\n        \"scaler\": scaler,\n        \"feature_names\": available_features\n    }\n\nprint(\"Preprocessing helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for all 3 feature sets\ndata_A = prepare_data(df, FEATURES_A, TARGET)\ndata_B = prepare_data(df, FEATURES_B, TARGET)\ndata_C = prepare_data(df, FEATURES_C, TARGET)\n\ndatasets = {\"A\": data_A, \"B\": data_B, \"C\": data_C}\n\nfor name, data in datasets.items():\n    print(f\"\\nFeature Set {name}:\")\n    print(f\"  Features: {len(data['feature_names'])}\")\n    print(f\"  Training samples: {len(data['y_train'])}\")\n    print(f\"  Validation samples: {len(data['y_val'])}\")\n    print(f\"  Test samples: {len(data['y_test'])}\")\n    print(f\"  Default rate (train): {data['y_train'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n# Section 4: Model Training (15 Variants)\n\nTrain 5 algorithms \u00d7 3 feature sets = 15 model variants:\n1. Logistic Regression\n2. Random Forest\n3. XGBoost\n4. LightGBM\n5. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ks_statistic(y_true, y_prob):\n    \"\"\"Calculate Kolmogorov-Smirnov statistic.\"\"\"\n    prob_default = y_prob[y_true == 1]\n    prob_paid = y_prob[y_true == 0]\n    ks_stat, _ = stats.ks_2samp(prob_default, prob_paid)\n    return ks_stat\n\n\ndef evaluate_model(model, X_test, y_test, model_name, feature_set):\n    \"\"\"Evaluate model and return all metrics.\"\"\"\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n    \n    metrics = {\n        \"Model\": model_name,\n        \"Feature_Set\": feature_set,\n        \"Accuracy\": accuracy_score(y_test, y_pred),\n        \"Precision\": precision_score(y_test, y_pred),\n        \"Recall\": recall_score(y_test, y_pred),\n        \"F1-Score\": f1_score(y_test, y_pred),\n        \"ROC-AUC\": roc_auc_score(y_test, y_prob),\n        \"KS Statistic\": calculate_ks_statistic(y_test, y_prob)\n    }\n    \n    return metrics, y_pred, y_prob\n\nprint(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with hyperparameter search spaces\ndef get_models_config(scale_pos_weight):\n    return {\n        \"Logistic Regression\": {\n            \"model\": LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=RANDOM_STATE),\n            \"params\": {\"C\": [0.01, 0.1, 1, 10], \"penalty\": [\"l2\"]}\n        },\n        \"Random Forest\": {\n            \"model\": RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n            \"params\": {\n                \"n_estimators\": [100, 200],\n                \"max_depth\": [5, 10, 15, None],\n                \"min_samples_split\": [2, 5, 10],\n                \"min_samples_leaf\": [1, 2, 4]\n            }\n        },\n        \"XGBoost\": {\n            \"model\": XGBClassifier(\n                scale_pos_weight=scale_pos_weight,\n                random_state=RANDOM_STATE,\n                eval_metric=\"logloss\",\n                use_label_encoder=False\n            ),\n            \"params\": {\n                \"n_estimators\": [100, 200],\n                \"max_depth\": [3, 5, 7],\n                \"learning_rate\": [0.01, 0.1, 0.2],\n                \"subsample\": [0.8, 1.0],\n                \"colsample_bytree\": [0.8, 1.0]\n            }\n        },\n        \"LightGBM\": {\n            \"model\": LGBMClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, verbose=-1),\n            \"params\": {\n                \"n_estimators\": [100, 200],\n                \"max_depth\": [3, 5, 7, -1],\n                \"learning_rate\": [0.01, 0.1, 0.2],\n                \"num_leaves\": [31, 50, 100]\n            }\n        },\n        \"CatBoost\": {\n            \"model\": CatBoostClassifier(auto_class_weights=\"Balanced\", random_state=RANDOM_STATE, verbose=0),\n            \"params\": {\n                \"iterations\": [100, 200],\n                \"depth\": [4, 6, 8],\n                \"learning_rate\": [0.01, 0.1, 0.2]\n            }\n        }\n    }\n\nprint(\"Model configurations defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-all-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all 15 model variants\nall_results = []\nall_models = {}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nfor feature_set_name, data in datasets.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"TRAINING FEATURE SET {feature_set_name} ({len(data['feature_names'])} features)\")\n    print(f\"{'='*70}\")\n    \n    # Apply SMOTE to training data\n    smote = SMOTE(random_state=RANDOM_STATE)\n    X_train_smote, y_train_smote = smote.fit_resample(data[\"X_train\"], data[\"y_train\"])\n    print(f\"SMOTE: {len(data['y_train'])} -> {len(y_train_smote)} samples\")\n    \n    # Calculate class weight\n    scale_pos_weight = (len(data[\"y_train\"]) - data[\"y_train\"].sum()) / data[\"y_train\"].sum()\n    models_config = get_models_config(scale_pos_weight)\n    \n    for model_name, config in models_config.items():\n        print(f\"\\n  Training: {model_name}...\")\n        \n        # Randomized search\n        search = RandomizedSearchCV(\n            config[\"model\"],\n            config[\"params\"],\n            n_iter=10,\n            cv=cv,\n            scoring=\"roc_auc\",\n            random_state=RANDOM_STATE,\n            n_jobs=-1\n        )\n        \n        search.fit(X_train_smote, y_train_smote)\n        best_model = search.best_estimator_\n        \n        # Store model\n        model_key = f\"{model_name}_{feature_set_name}\"\n        all_models[model_key] = best_model\n        \n        # Evaluate on test set\n        metrics, y_pred, y_prob = evaluate_model(\n            best_model, data[\"X_test\"], data[\"y_test\"], model_name, feature_set_name\n        )\n        all_results.append(metrics)\n        \n        print(f\"    CV ROC-AUC: {search.best_score_:.4f}\")\n        print(f\"    Test ROC-AUC: {metrics['ROC-AUC']:.4f}, KS: {metrics['KS Statistic']:.4f}\")\n\nprint(f\"\\n\\nTotal models trained: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n# Section 5: Evaluation & Comparison\n\nCompare all 15 model variants and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\nresults_df = pd.DataFrame(all_results)\nresults_df = results_df.sort_values(\"KS Statistic\", ascending=False)\n\nprint(\"=\" * 80)\nprint(\"MODEL COMPARISON - ALL 15 VARIANTS (Sorted by KS Statistic)\")\nprint(\"=\" * 80)\nprint(results_df.to_string(index=False))\n\n# Save results\nresults_df.to_csv(\"../models/v2/model_comparison_v2.csv\", index=False)\nprint(\"\\nResults saved to models/v2/model_comparison_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-feature-sets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare by feature set\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PERFORMANCE BY FEATURE SET\")\nprint(\"=\" * 60)\n\nfeature_set_comparison = results_df.groupby(\"Feature_Set\").agg({\n    \"ROC-AUC\": [\"mean\", \"max\"],\n    \"KS Statistic\": [\"mean\", \"max\"],\n    \"F1-Score\": [\"mean\", \"max\"]\n}).round(4)\n\nprint(feature_set_comparison)\n\n# Which feature set performs best?\nbest_by_ks = results_df.loc[results_df.groupby(\"Feature_Set\")[\"KS Statistic\"].idxmax()]\nprint(\"\\nBest model per feature set (by KS):\")\nprint(best_by_ks[[\"Feature_Set\", \"Model\", \"ROC-AUC\", \"KS Statistic\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-algorithms",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare by algorithm\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PERFORMANCE BY ALGORITHM\")\nprint(\"=\" * 60)\n\nalgorithm_comparison = results_df.groupby(\"Model\").agg({\n    \"ROC-AUC\": [\"mean\", \"max\"],\n    \"KS Statistic\": [\"mean\", \"max\"],\n    \"F1-Score\": [\"mean\", \"max\"]\n}).round(4)\n\nprint(algorithm_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-model-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model overall\nbest_idx = results_df[\"KS Statistic\"].idxmax()\nbest_row = results_df.loc[best_idx]\nbest_model_name = best_row[\"Model\"]\nbest_feature_set = best_row[\"Feature_Set\"]\nbest_model_key = f\"{best_model_name}_{best_feature_set}\"\nbest_model = all_models[best_model_key]\n\nprint(\"=\" * 60)\nprint(\"BEST MODEL SELECTED\")\nprint(\"=\" * 60)\nprint(f\"\\nModel: {best_model_name}\")\nprint(f\"Feature Set: {best_feature_set}\")\nprint(f\"\\nMetrics:\")\nfor col in [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\", \"KS Statistic\"]:\n    print(f\"  {col}: {best_row[col]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline\nprint(\"\\n\" + \"=\" * 60)\nprint(\"IMPROVEMENT OVER BASELINE\")\nprint(\"=\" * 60)\n\nbaseline_ks = 0.21\nbaseline_auc = 0.60\nbest_ks = best_row[\"KS Statistic\"]\nbest_auc = best_row[\"ROC-AUC\"]\n\nks_improvement = ((best_ks - baseline_ks) / baseline_ks) * 100\nauc_improvement = ((best_auc - baseline_auc) / baseline_auc) * 100\n\nprint(f\"\\n| Metric | Baseline (v1) | Improved (v2) | Change |\")\nprint(f\"|--------|---------------|---------------|--------|\")\nprint(f\"| KS Statistic | {baseline_ks:.2f} | {best_ks:.4f} | {'+' if ks_improvement > 0 else ''}{ks_improvement:.1f}% |\")\nprint(f\"| ROC-AUC | {baseline_auc:.2f} | {best_auc:.4f} | {'+' if auc_improvement > 0 else ''}{auc_improvement:.1f}% |\")\n\n# Check success criteria\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUCCESS CRITERIA CHECK\")\nprint(\"=\" * 60)\ncriteria = [\n    (\"KS Statistic >= 0.28\", best_ks >= 0.28, best_ks),\n    (\"ROC-AUC >= 0.68\", best_auc >= 0.68, best_auc),\n]\nfor criterion, passed, value in criteria:\n    status = \"\u2713 PASSED\" if passed else \"\u2717 NOT MET\"\n    print(f\"{criterion}: {value:.4f} - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n# Section 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-ks-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: All 15 models sorted by KS statistic\nplt.figure(figsize=(14, 8))\n\ncolors = {\"A\": \"blue\", \"B\": \"green\", \"C\": \"orange\"}\nbar_colors = [colors[row[\"Feature_Set\"]] for _, row in results_df.iterrows()]\n\nbars = plt.barh(\n    range(len(results_df)),\n    results_df[\"KS Statistic\"],\n    color=bar_colors\n)\n\nplt.axvline(x=0.21, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Baseline KS (0.21)\")\nplt.axvline(x=0.28, color=\"darkgreen\", linestyle=\"--\", linewidth=2, label=\"Target KS (0.28)\")\n\n# Labels\nlabels = [f\"{row['Model']} ({row['Feature_Set']})\" for _, row in results_df.iterrows()]\nplt.yticks(range(len(results_df)), labels)\nplt.xlabel(\"KS Statistic\")\nplt.title(\"Model Comparison - KS Statistic (All 15 Variants)\")\nplt.legend()\n\n# Add value labels\nfor i, (idx, row) in enumerate(results_df.iterrows()):\n    plt.text(row[\"KS Statistic\"] + 0.005, i, f'{row[\"KS Statistic\"]:.3f}', va=\"center\")\n\nplt.tight_layout()\nplt.savefig(\"../models/v2/ks_comparison_all_models.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-feature-set-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart: Feature sets x Algorithms\nfig, ax = plt.subplots(figsize=(14, 8))\n\nalgorithms = results_df[\"Model\"].unique()\nfeature_sets = [\"A\", \"B\", \"C\"]\nx = np.arange(len(algorithms))\nwidth = 0.25\n\nfor i, fs in enumerate(feature_sets):\n    fs_data = results_df[results_df[\"Feature_Set\"] == fs]\n    ks_values = [fs_data[fs_data[\"Model\"] == algo][\"KS Statistic\"].values[0] \n                 if len(fs_data[fs_data[\"Model\"] == algo]) > 0 else 0 \n                 for algo in algorithms]\n    ax.bar(x + i*width, ks_values, width, label=f\"Feature Set {fs}\", color=colors[fs])\n\nax.axhline(y=0.21, color=\"red\", linestyle=\"--\", label=\"Baseline (0.21)\")\nax.axhline(y=0.28, color=\"darkgreen\", linestyle=\"--\", label=\"Target (0.28)\")\nax.set_xlabel(\"Algorithm\")\nax.set_ylabel(\"KS Statistic\")\nax.set_title(\"KS Statistic by Feature Set and Algorithm\")\nax.set_xticks(x + width)\nax.set_xticklabels(algorithms, rotation=45, ha=\"right\")\nax.legend()\nax.grid(True, alpha=0.3, axis=\"y\")\n\nplt.tight_layout()\nplt.savefig(\"../models/v2/feature_set_algorithm_comparison.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves - All 15 models\nplt.figure(figsize=(12, 10))\n\n# Get predictions for all models\nfor model_key, model in all_models.items():\n    parts = model_key.rsplit(\"_\", 1)\n    model_name = parts[0]\n    feature_set = parts[1]\n    \n    data = datasets[feature_set]\n    y_prob = model.predict_proba(data[\"X_test\"])[:, 1]\n    fpr, tpr, _ = roc_curve(data[\"y_test\"], y_prob)\n    auc = roc_auc_score(data[\"y_test\"], y_prob)\n    \n    plt.plot(fpr, tpr, label=f\"{model_name} ({feature_set}) AUC={auc:.3f}\", \n             linewidth=1.5, alpha=0.8)\n\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier\")\nplt.axhline(y=0.60, color=\"red\", linestyle=\":\", alpha=0.5, label=\"Baseline AUC (0.60)\")\nplt.xlabel(\"False Positive Rate\", fontsize=12)\nplt.ylabel(\"True Positive Rate\", fontsize=12)\nplt.title(\"ROC Curves - All 15 Model Variants\", fontsize=14)\nplt.legend(loc=\"lower right\", fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"../models/v2/roc_curves_all_models.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for top 3 models\ntop3 = results_df.head(3)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor idx, (_, row) in enumerate(top3.iterrows()):\n    model_key = f\"{row['Model']}_{row['Feature_Set']}\"\n    model = all_models[model_key]\n    data = datasets[row[\"Feature_Set\"]]\n    \n    y_pred = model.predict(data[\"X_test\"])\n    cm = confusion_matrix(data[\"y_test\"], y_pred)\n    \n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[idx],\n                xticklabels=[\"Paid\", \"Defaulted\"],\n                yticklabels=[\"Paid\", \"Defaulted\"])\n    axes[idx].set_title(f\"{row['Model']} ({row['Feature_Set']})\\nKS={row['KS Statistic']:.3f}\")\n    axes[idx].set_xlabel(\"Predicted\")\n    axes[idx].set_ylabel(\"Actual\")\n\nplt.suptitle(\"Confusion Matrices - Top 3 Models\", fontsize=14)\nplt.tight_layout()\nplt.savefig(\"../models/v2/confusion_matrices_top3.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "---\n# Section 7: Best Model Selection & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and artifacts\nbest_data = datasets[best_feature_set]\n\n# Save best model\njoblib.dump(best_model, \"../models/v2/best_loan_default_model_v2.pkl\")\nprint(f\"Best model saved: models/v2/best_loan_default_model_v2.pkl\")\n\n# Save scaler\njoblib.dump(best_data[\"scaler\"], \"../models/v2/scaler_v2.pkl\")\nprint(f\"Scaler saved: models/v2/scaler_v2.pkl\")\n\n# Save encoders\njoblib.dump(best_data[\"encoders\"], \"../models/v2/encoders_v2.pkl\")\nprint(f\"Encoders saved: models/v2/encoders_v2.pkl\")\n\n# Save feature list\njoblib.dump(best_data[\"feature_names\"], \"../models/v2/feature_list_v2.pkl\")\nprint(f\"Feature list saved: models/v2/feature_list_v2.pkl\")\n\n# Save all trained models\njoblib.dump(all_models, \"../models/v2/all_trained_models_v2.pkl\")\nprint(f\"All models saved: models/v2/all_trained_models_v2.pkl\")\n\n# Save composite score functions as metadata\nmetadata = {\n    \"best_model\": best_model_name,\n    \"best_feature_set\": best_feature_set,\n    \"feature_names\": best_data[\"feature_names\"],\n    \"baseline_ks\": 0.21,\n    \"baseline_auc\": 0.60,\n    \"best_ks\": float(best_row[\"KS Statistic\"]),\n    \"best_auc\": float(best_row[\"ROC-AUC\"])\n}\njoblib.dump(metadata, \"../models/v2/model_metadata_v2.pkl\")\nprint(f\"Metadata saved: models/v2/model_metadata_v2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-header",
   "metadata": {},
   "source": [
    "---\n# Section 8: Model Interpretation (SHAP, Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best model\nif hasattr(best_model, \"feature_importances_\"):\n    importance_df = pd.DataFrame({\n        \"Feature\": best_data[\"feature_names\"],\n        \"Importance\": best_model.feature_importances_\n    }).sort_values(\"Importance\", ascending=False)\n    \n    print(\"=\" * 50)\n    print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n    print(\"=\" * 50)\n    print(importance_df.to_string(index=False))\n    \n    # Visualization\n    plt.figure(figsize=(10, 6))\n    plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"steelblue\")\n    plt.xlabel(\"Importance\")\n    plt.title(f\"Feature Importance - {best_model_name} (Feature Set {best_feature_set})\")\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.savefig(\"../models/v2/feature_importance_best_model.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\nelse:\n    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for best model\nprint(f\"\\nGenerating SHAP values for {best_model_name}...\")\n\ntry:\n    if best_model_name in [\"Random Forest\", \"XGBoost\", \"LightGBM\", \"CatBoost\"]:\n        explainer = shap.TreeExplainer(best_model)\n        shap_values = explainer.shap_values(best_data[\"X_test\"])\n        \n        if isinstance(shap_values, list):\n            shap_values = shap_values[1]\n    else:\n        explainer = shap.LinearExplainer(best_model, best_data[\"X_train\"])\n        shap_values = explainer.shap_values(best_data[\"X_test\"])\n    \n    # Summary plot\n    plt.figure(figsize=(12, 8))\n    shap.summary_plot(shap_values, best_data[\"X_test\"], \n                      feature_names=best_data[\"feature_names\"], show=False)\n    plt.title(f\"SHAP Feature Importance - {best_model_name}\")\n    plt.tight_layout()\n    plt.savefig(\"../models/v2/shap_summary.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    \n    # Bar plot\n    plt.figure(figsize=(10, 6))\n    shap.summary_plot(shap_values, best_data[\"X_test\"], \n                      feature_names=best_data[\"feature_names\"], \n                      plot_type=\"bar\", show=False)\n    plt.title(f\"Mean |SHAP| - {best_model_name}\")\n    plt.tight_layout()\n    plt.savefig(\"../models/v2/shap_bar.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    \n    print(\"SHAP analysis complete.\")\nexcept Exception as e:\n    print(f\"SHAP analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9-header",
   "metadata": {},
   "source": [
    "---\n# Section 9: Updated Prediction Function\n\nA deployment-ready prediction function that uses composite scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loan_default_v2(borrower_features, \n                            model_path=\"../models/v2/best_loan_default_model_v2.pkl\",\n                            scaler_path=\"../models/v2/scaler_v2.pkl\",\n                            encoders_path=\"../models/v2/encoders_v2.pkl\",\n                            features_path=\"../models/v2/feature_list_v2.pkl\"):\n    \"\"\"\n    Improved prediction function using composite scores.\n    \n    Parameters:\n    -----------\n    borrower_features : dict\n        Can include either:\n        - Raw features (will calculate composite scores)\n        - Pre-calculated composite scores\n        \n    Returns:\n    --------\n    dict with:\n        - default_probability: float\n        - payment_probability: float\n        - risk_category: str\n        - composite_scores: dict\n        - recommendation: str\n        - confidence: str\n    \"\"\"\n    # Load model and preprocessors\n    model = joblib.load(model_path)\n    scaler = joblib.load(scaler_path)\n    encoders = joblib.load(encoders_path)\n    feature_list = joblib.load(features_path)\n    \n    # Calculate composite scores if not provided\n    if \"Financial_Resilience_Score\" not in borrower_features:\n        # Create intermediate features first\n        borrower_features[\"Extra_Income_Brackets\"] = borrower_features.get(\"Extra Income Brackets\", \"No Extra Income\")\n        borrower_features[\"Expense_Ratio\"] = borrower_features.get(\"Expense Relative to Income\", \"Unknown\")\n        borrower_features[\"Income_Diversity\"] = borrower_features.get(\"Logic on Income\", \"Income Only\")\n        borrower_features[\"Savings_Category\"] = borrower_features.get(\"Savings Categorical\", \"No Savings\")\n        borrower_features[\"Rent_Category\"] = borrower_features.get(\"Categorize Rent Payment\", \"Unknown\")\n        borrower_features[\"Utility_Category\"] = borrower_features.get(\"Categorizing Utility Expenses\", \"Unknown\")\n        borrower_features[\"Affordability_Business\"] = borrower_features.get(\"Affordability\", \"Unknown\")\n        borrower_features[\"Affordability_HH\"] = borrower_features.get(\"Affordability (HH)\", \"Unknown\")\n        borrower_features[\"SchoolFees_Category\"] = borrower_features.get(\"School Fees Categorical\", \"Unknown\")\n        borrower_features[\"Regular_Income_Brackets\"] = borrower_features.get(\"Regular Income Brackets\", \"No Regular Income\")\n        \n        # Calculate composite scores\n        borrower_features[\"Financial_Resilience_Score\"] = calculate_financial_resilience(borrower_features)\n        borrower_features[\"Business_Quality_Score\"] = calculate_business_quality(borrower_features)\n        borrower_features[\"Stability_Score\"] = calculate_stability(borrower_features)\n        borrower_features[\"Expense_Management_Score\"] = calculate_expense_management(borrower_features)\n    \n    # Create feature vector\n    X = pd.DataFrame([borrower_features])\n    \n    # Ensure all features are present\n    for feature in feature_list:\n        if feature not in X.columns:\n            X[feature] = \"Unknown\"\n    \n    # Reorder columns\n    X = X[feature_list]\n    \n    # Encode categorical features\n    for col in feature_list:\n        if col in encoders:\n            le = encoders[col]\n            val = str(X[col].iloc[0])\n            if val in le.classes_:\n                X[col] = le.transform([val])[0]\n            else:\n                X[col] = 0\n    \n    # Scale features\n    X_scaled = scaler.transform(X)\n    \n    # Predict\n    probability = model.predict_proba(X_scaled)[0, 1]\n    \n    # Determine risk category and confidence\n    if probability < 0.25:\n        risk_category = \"Low Risk\"\n        recommendation = \"APPROVE - Low default risk. Standard loan terms recommended.\"\n        confidence = \"High\"\n    elif probability < 0.40:\n        risk_category = \"Medium-Low Risk\"\n        recommendation = \"APPROVE WITH MONITORING - Moderate-low risk. Standard terms with periodic check-ins.\"\n        confidence = \"Medium\"\n    elif probability < 0.55:\n        risk_category = \"Medium Risk\"\n        recommendation = \"REVIEW - Moderate default risk. Consider reduced loan amount or additional guarantor.\"\n        confidence = \"Medium\"\n    elif probability < 0.70:\n        risk_category = \"Medium-High Risk\"\n        recommendation = \"CAUTION - Elevated risk. Require strong collateral or reduced amount.\"\n        confidence = \"Medium\"\n    else:\n        risk_category = \"High Risk\"\n        recommendation = \"DECLINE or SPECIAL TERMS - High default risk. Recommend declining or special conditions.\"\n        confidence = \"High\"\n    \n    return {\n        \"default_probability\": round(probability, 4),\n        \"payment_probability\": round(1 - probability, 4),\n        \"risk_category\": risk_category,\n        \"composite_scores\": {\n            \"Financial Resilience\": borrower_features.get(\"Financial_Resilience_Score\", \"N/A\"),\n            \"Business Quality\": borrower_features.get(\"Business_Quality_Score\", \"N/A\"),\n            \"Stability\": borrower_features.get(\"Stability_Score\", \"N/A\"),\n            \"Expense Management\": borrower_features.get(\"Expense_Management_Score\", \"N/A\")\n        },\n        \"recommendation\": recommendation,\n        \"confidence\": confidence\n    }\n\nprint(\"Prediction function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with 3 sample borrowers\nprint(\"=\" * 70)\nprint(\"SAMPLE PREDICTIONS\")\nprint(\"=\" * 70)\n\n# Sample 1: Low-risk borrower\nlow_risk = {\n    \"Extra Income Brackets\": \"Moderate to High Extra Income\",\n    \"Categorize Rent Payment\": \"High Rent\",\n    \"School Fees Categorical\": \"High School Fees\",\n    \"Age Group\": \"Mid Life 40-49\",\n    \"Education\": \"Tertiary level (Colleges, Universities, Polytechnics)\",\n    \"CRB Class\": \"Active Low-Medium Risk\",\n    \"Living\": \"Peri-Urban\",\n    \"Logic on Income\": \"Income + Extra + Regular (Full Diversity)\",\n    \"Categorizing Utility Expenses\": \"High Utility Expenses\",\n    \"Expense Relative to Income\": \"1/3 or Less of Income\",\n    \"Affordability (HH)\": \"Profitable (Affordable)\",\n    \"Marital status\": \"Married\"\n}\n\nprint(\"\\nSample 1: Low-Risk Borrower Profile\")\nresult1 = predict_loan_default_v2(low_risk)\nprint(f\"  Default Probability: {result1['default_probability']:.2%}\")\nprint(f\"  Risk Category: {result1['risk_category']}\")\nprint(f\"  Composite Scores: {result1['composite_scores']}\")\nprint(f\"  Recommendation: {result1['recommendation']}\")\n\n# Sample 2: High-risk borrower\nhigh_risk = {\n    \"Extra Income Brackets\": \"No Extra Income\",\n    \"Categorize Rent Payment\": \"Low Rent\",\n    \"School Fees Categorical\": \"No School Fees\",\n    \"Age Group\": \"Young Adults 21-29\",\n    \"Education\": \"Secondary Incomplete\",\n    \"CRB Class\": \"Legacy\",\n    \"Living\": \"Urban\",\n    \"Logic on Income\": \"Income Only\",\n    \"Categorizing Utility Expenses\": \"No Utility Expenses\",\n    \"Expense Relative to Income\": \"More than 2/3 of Income\",\n    \"Affordability (HH)\": \"Low/Negative Profit (Unviable)\",\n    \"Marital status\": \"Single\"\n}\n\nprint(\"\\nSample 2: High-Risk Borrower Profile\")\nresult2 = predict_loan_default_v2(high_risk)\nprint(f\"  Default Probability: {result2['default_probability']:.2%}\")\nprint(f\"  Risk Category: {result2['risk_category']}\")\nprint(f\"  Composite Scores: {result2['composite_scores']}\")\nprint(f\"  Recommendation: {result2['recommendation']}\")\n\n# Sample 3: Medium-risk borrower\nmedium_risk = {\n    \"Extra Income Brackets\": \"Low Extra Income\",\n    \"Categorize Rent Payment\": \"Low Rent\",\n    \"School Fees Categorical\": \"Low School Fees\",\n    \"Age Group\": \"Early Mature 30-39\",\n    \"Education\": \"Secondary Complete\",\n    \"CRB Class\": \"Active High-Medium High Risk\",\n    \"Living\": \"Peri-Urban\",\n    \"Logic on Income\": \"Income + Extra\",\n    \"Categorizing Utility Expenses\": \"Low Utility Expenses\",\n    \"Expense Relative to Income\": \"Half of Income\",\n    \"Affordability (HH)\": \"Profitable (Affordable)\",\n    \"Marital status\": \"Married\"\n}\n\nprint(\"\\nSample 3: Medium-Risk Borrower Profile\")\nresult3 = predict_loan_default_v2(medium_risk)\nprint(f\"  Default Probability: {result3['default_probability']:.2%}\")\nprint(f\"  Risk Category: {result3['risk_category']}\")\nprint(f\"  Composite Scores: {result3['composite_scores']}\")\nprint(f\"  Recommendation: {result3['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10-header",
   "metadata": {},
   "source": [
    "---\n# Section 10: Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\nprint(\"MODEL PERFORMANCE SUMMARY\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\n# Model Performance Summary\n\n## Baseline vs. Improved Model\n\"\"\")\n\nprint(f\"| Metric | Baseline (v1) | Improved (v2) | Change |\")\nprint(f\"|--------|---------------|---------------|--------|\")\nprint(f\"| KS Statistic | 0.21 | {best_row['KS Statistic']:.4f} | {'+' if ks_improvement > 0 else ''}{ks_improvement:.1f}% |\")\nprint(f\"| ROC-AUC | 0.60 | {best_row['ROC-AUC']:.4f} | {'+' if auc_improvement > 0 else ''}{auc_improvement:.1f}% |\")\nprint(f\"| Features Used | 12 raw | {len(best_data['feature_names'])} composite+select | Simplified |\")\n\nprint(f\"\"\"\n## Best Performing Configuration\n\n- **Feature Set**: {best_feature_set}\n- **Algorithm**: {best_model_name}\n- **Features**: {best_data['feature_names']}\n\n## Performance Metrics\n\"\"\")\nfor col in [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\", \"KS Statistic\"]:\n    print(f\"- {col}: {best_row[col]:.4f}\")\n\nprint(\"\"\"\n## Key Findings\n\n1. **Composite Score Impact**: Using engineered composite scores improved model performance\n2. **Feature Set Comparison**: Tested 3 variations (A, B, C) with different feature counts\n3. **Algorithm Performance**: Compared 5 algorithms to find optimal combination\n4. **Prior Loan Access**: Feature Set C excludes this to test impact\n\n## Recommendations\n\n- Deploy Model v2 with the selected feature set\n- Use composite scores as primary risk indicators\n- Monitor performance and retrain periodically\n\"\"\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ARTIFACTS SAVED\")\nprint(\"=\" * 70)\nprint(\"\"\"\n- models/v2/best_loan_default_model_v2.pkl\n- models/v2/scaler_v2.pkl\n- models/v2/encoders_v2.pkl\n- models/v2/feature_list_v2.pkl\n- models/v2/all_trained_models_v2.pkl\n- models/v2/model_metadata_v2.pkl\n- models/v2/model_comparison_v2.csv\n- models/v2/composite_score_distributions.png\n- models/v2/ks_comparison_all_models.png\n- models/v2/feature_set_algorithm_comparison.png\n- models/v2/roc_curves_all_models.png\n- models/v2/confusion_matrices_top3.png\n- models/v2/feature_importance_best_model.png\n- models/v2/shap_summary.png\n- models/v2/shap_bar.png\n\"\"\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"READY FOR DEPLOYMENT\")\nprint(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}