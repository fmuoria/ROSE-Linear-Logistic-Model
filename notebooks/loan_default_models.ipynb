{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# ROSE Women's Foundation - Loan Default Prediction Models\n",
    "## Comprehensive ML Pipeline for Default Risk Assessment\n",
    "\n",
    "This notebook implements:\n",
    "1. Data Loading & Preprocessing\n",
    "2. Multiple Model Training (Logistic Regression, Random Forest, XGBoost, LightGBM, CatBoost)\n",
    "3. Evaluation Metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC, KS Statistic)\n",
    "4. Model Comparison & Selection\n",
    "5. Model Saving & Prediction Function\n",
    "6. Model Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Interpretability\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (handle encoding issues)\n",
    "# Data path is relative to notebooks directory; adjust if running from different location\n",
    "DATA_PATH = '../Github Original Data.csv'\n",
    "df = pd.read_csv(DATA_PATH, encoding='latin-1')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget variable 'Defaulted' distribution:\")\n",
    "print(df['Defaulted'].value_counts())\n",
    "print(f\"\\nDefault rate: {df['Defaulted'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features based on EDA findings\n",
    "# Tier 1 (Priority) + Tier 2 features\n",
    "FEATURE_COLUMNS = [\n",
    "    # Tier 1 - Strong predictors\n",
    "    'Extra Income Brackets',\n",
    "    'Categorize Rent Payment',\n",
    "    'School Fees Categorical',\n",
    "    'Age Group',\n",
    "    'Education',\n",
    "    'Loan Access',  # Prior Loan Access\n",
    "    'CRB Class',\n",
    "    # Tier 2 - Moderate predictors\n",
    "    'Logic on Income',  # Income Diversity\n",
    "    'Categorizing Utility Expenses',\n",
    "    'Expense Relative to Income',\n",
    "    'Affordability (HH)',\n",
    "    'Living',\n",
    "]\n",
    "\n",
    "TARGET = 'Defaulted'\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in FEATURE_COLUMNS if f in df.columns]\n",
    "missing_features = [f for f in FEATURE_COLUMNS if f not in df.columns]\n",
    "\n",
    "print(f\"Available features ({len(available_features)}): {available_features}\")\n",
    "print(f\"\\nMissing features ({len(missing_features)}): {missing_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working dataset with available features\n",
    "df_model = df[available_features + [TARGET]].copy()\n",
    "print(f\"Working dataset shape: {df_model.shape}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df_model.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For categorical: fill with mode\n",
    "# For numerical: fill with median\n",
    "\n",
    "for col in df_model.columns:\n",
    "    if col == TARGET:\n",
    "        continue\n",
    "    if df_model[col].dtype == 'object':\n",
    "        mode_val = df_model[col].mode()[0] if len(df_model[col].mode()) > 0 else 'Unknown'\n",
    "        df_model[col] = df_model[col].fillna(mode_val)\n",
    "    else:\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_model.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encode-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using LabelEncoder\n",
    "encoders = {}\n",
    "df_encoded = df_model.copy()\n",
    "\n",
    "for col in available_features:\n",
    "    if df_encoded[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        encoders[col] = le\n",
    "        print(f\"Encoded {col}: {len(le.classes_)} classes\")\n",
    "\n",
    "print(f\"\\nTotal encoders created: {len(encoders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded[available_features]\n",
    "y = df_encoded[TARGET]\n",
    "\n",
    "# Train/Val/Test split: 70/15/15 (stratified)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")  # 0.176 of 0.85 = ~0.15 of total\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% default)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.1f}% default)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for tree-based models (they don't need scaling but we keep column names)\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=available_features, index=X_train.index)\n",
    "X_val_df = pd.DataFrame(X_val_scaled, columns=available_features, index=X_val.index)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=available_features, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smote",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training data to handle class imbalance\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set: {len(y_train)} samples\")\n",
    "print(f\"After SMOTE: {len(y_train_smote)} samples\")\n",
    "print(f\"Class distribution after SMOTE: {np.bincount(y_train_smote)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ks-statistic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ks_statistic(y_true, y_prob):\n",
    "    \"\"\"Calculate Kolmogorov-Smirnov statistic.\"\"\"\n",
    "    # Separate probabilities by class\n",
    "    prob_default = y_prob[y_true == 1]\n",
    "    prob_paid = y_prob[y_true == 0]\n",
    "    \n",
    "    # Use scipy's KS test\n",
    "    ks_stat, _ = stats.ks_2samp(prob_default, prob_paid)\n",
    "    return ks_stat\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return all metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob),\n",
    "        'KS Statistic': calculate_ks_statistic(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Define models with their hyperparameter search spaces\n",
    "models_config = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'use_smote': False  # Uses class_weight instead\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'use_smote': False\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        },\n",
    "        'use_smote': False\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7, -1],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'num_leaves': [31, 50, 100]\n",
    "        },\n",
    "        'use_smote': False\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'model': CatBoostClassifier(\n",
    "            auto_class_weights='Balanced',\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=0\n",
    "        ),\n",
    "        'params': {\n",
    "            'iterations': [100, 200],\n",
    "            'depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.1, 0.2]\n",
    "        },\n",
    "        'use_smote': False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Models to train: {list(models_config.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with RandomizedSearchCV\n",
    "trained_models = {}\n",
    "all_metrics = []\n",
    "all_predictions = {}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Select data based on whether to use SMOTE\n",
    "    if config['use_smote']:\n",
    "        X_fit, y_fit = X_train_smote, y_train_smote\n",
    "    else:\n",
    "        X_fit, y_fit = X_train_scaled, y_train\n",
    "    \n",
    "    # Randomized search for hyperparameter tuning\n",
    "    search = RandomizedSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        n_iter=10,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_fit, y_fit)\n",
    "    \n",
    "    best_model = search.best_estimator_\n",
    "    trained_models[name] = best_model\n",
    "    \n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "    print(f\"Best CV ROC-AUC: {search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    metrics, y_pred, y_prob = evaluate_model(best_model, X_test_scaled, y_test, name)\n",
    "    all_metrics.append(metrics)\n",
    "    all_predictions[name] = {'y_pred': y_pred, 'y_prob': y_prob}\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Evaluation Metrics & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, preds) in enumerate(all_predictions.items()):\n",
    "    cm = confusion_matrix(y_test, preds['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Paid', 'Defaulted'],\n",
    "                yticklabels=['Paid', 'Defaulted'])\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "# Hide empty subplot\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, preds in all_predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds['y_prob'])\n",
    "    auc = roc_auc_score(y_test, preds['y_prob'])\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - All Models', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../models/roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pr-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, preds in all_predictions.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, preds['y_prob'])\n",
    "    plt.plot(recall, precision, label=f'{name}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves - All Models', fontsize=14)\n",
    "plt.legend(loc='lower left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../models/precision_recall_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for tree-based models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "tree_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "\n",
    "for idx, name in enumerate(tree_models):\n",
    "    model = trained_models[name]\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = np.zeros(len(available_features))\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    axes[idx].barh(range(len(available_features)), importances[indices], color='steelblue')\n",
    "    axes[idx].set_yticks(range(len(available_features)))\n",
    "    axes[idx].set_yticklabels([available_features[i] for i in indices])\n",
    "    axes[idx].set_xlabel('Importance')\n",
    "    axes[idx].set_title(f'{name} - Feature Importance')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "# Round for display\n",
    "comparison_display = comparison_df.round(4)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON - ALL METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_display.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('../models/model_comparison.csv')\n",
    "print(\"\\nComparison saved to models/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'KS Statistic']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.15\n",
    "\n",
    "for i, model in enumerate(comparison_df.index):\n",
    "    values = [comparison_df.loc[model, m] for m in metrics_to_plot]\n",
    "    ax.bar(x + i*width, values, width, label=model)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Comparison Across All Metrics', fontsize=14)\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(metrics_to_plot, fontsize=11)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.axhline(y=0.75, color='red', linestyle='--', alpha=0.5, label='ROC-AUC Target')\n",
    "ax.axhline(y=0.70, color='orange', linestyle='--', alpha=0.5, label='F1 Target')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_comparison_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on ROC-AUC\n",
    "best_model_name = comparison_df['ROC-AUC'].idxmax()\n",
    "best_model = trained_models[best_model_name]\n",
    "best_metrics = comparison_df.loc[best_model_name]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL SELECTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Check against success criteria\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS CRITERIA CHECK\")\n",
    "print(\"=\"*60)\n",
    "criteria = [\n",
    "    ('ROC-AUC >= 0.75', best_metrics['ROC-AUC'] >= 0.75, best_metrics['ROC-AUC']),\n",
    "    ('F1-Score >= 0.70', best_metrics['F1-Score'] >= 0.70, best_metrics['F1-Score']),\n",
    "    ('KS Statistic >= 0.35', best_metrics['KS Statistic'] >= 0.35, best_metrics['KS Statistic']),\n",
    "]\n",
    "\n",
    "for criterion, passed, value in criteria:\n",
    "    status = '✓ PASSED' if passed else '✗ NOT MET'\n",
    "    print(f\"{criterion}: {value:.4f} - {status}\")\n",
    "\n",
    "print(f\"\\nCompared 5 models: ✓ PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, '../models/best_loan_default_model.pkl')\n",
    "print(f\"Best model saved: models/best_loan_default_model.pkl\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(f\"Scaler saved: models/scaler.pkl\")\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(encoders, '../models/encoders.pkl')\n",
    "print(f\"Encoders saved: models/encoders.pkl\")\n",
    "\n",
    "# Save feature list\n",
    "joblib.dump(available_features, '../models/feature_list.pkl')\n",
    "print(f\"Feature list saved: models/feature_list.pkl\")\n",
    "\n",
    "# Save all trained models for comparison\n",
    "joblib.dump(trained_models, '../models/all_trained_models.pkl')\n",
    "print(f\"All models saved: models/all_trained_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loan_default(borrower_features, model_path='../models/best_loan_default_model.pkl',\n",
    "                         scaler_path='../models/scaler.pkl',\n",
    "                         encoders_path='../models/encoders.pkl',\n",
    "                         features_path='../models/feature_list.pkl'):\n",
    "    \"\"\"\n",
    "    Predict loan default probability for a borrower.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    borrower_features : dict\n",
    "        Dictionary with feature names as keys and values.\n",
    "        Example: {\n",
    "            'Extra Income Brackets': 'Low Extra Income',\n",
    "            'Categorize Rent Payment': 'High Rent',\n",
    "            'Age Group': 'Mid Life 40-49',\n",
    "            ...\n",
    "        }\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with:\n",
    "        - default_probability: float (0-1)\n",
    "        - risk_category: str ('Low', 'Medium', 'High')\n",
    "        - recommendation: str\n",
    "    \"\"\"\n",
    "    # Load model and preprocessors\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    encoders = joblib.load(encoders_path)\n",
    "    feature_list = joblib.load(features_path)\n",
    "    \n",
    "    # Create feature vector\n",
    "    X = pd.DataFrame([borrower_features])\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for feature in feature_list:\n",
    "        if feature not in X.columns:\n",
    "            X[feature] = 'Unknown'  # Default value\n",
    "    \n",
    "    # Reorder columns\n",
    "    X = X[feature_list]\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in feature_list:\n",
    "        if col in encoders:\n",
    "            le = encoders[col]\n",
    "            val = str(X[col].iloc[0])\n",
    "            if val in le.classes_:\n",
    "                X[col] = le.transform([val])[0]\n",
    "            else:\n",
    "                # Handle unseen category - use most frequent\n",
    "                X[col] = 0\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    probability = model.predict_proba(X_scaled)[0, 1]\n",
    "    \n",
    "    # Determine risk category\n",
    "    if probability < 0.3:\n",
    "        risk_category = 'Low Risk'\n",
    "        recommendation = 'APPROVE - Low default risk. Standard loan terms recommended.'\n",
    "    elif probability < 0.5:\n",
    "        risk_category = 'Medium Risk'\n",
    "        recommendation = 'REVIEW - Moderate default risk. Consider reduced loan amount or additional guarantor.'\n",
    "    else:\n",
    "        risk_category = 'High Risk'\n",
    "        recommendation = 'CAUTION - High default risk. Recommend declining or requiring strong collateral.'\n",
    "    \n",
    "    return {\n",
    "        'default_probability': round(probability, 4),\n",
    "        'risk_category': risk_category,\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "\n",
    "print(\"Prediction function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample borrowers\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample 1: Low-risk borrower profile\n",
    "low_risk_borrower = {\n",
    "    'Extra Income Brackets': 'Moderate to High Extra Income',\n",
    "    'Categorize Rent Payment': 'High Rent',\n",
    "    'School Fees Categorical': 'High School Fees',\n",
    "    'Age Group': 'Mid Life 40-49',\n",
    "    'Education': 'Tertiary level (Colleges, Universities, Polytechnics)',\n",
    "    'Loan Access': 'No',\n",
    "    'CRB Class': 'Active Low\\x96Medium Risk',\n",
    "    'Logic on Income': 'Income + Extra + Regular (Full Diversity)',\n",
    "    'Categorizing Utility Expenses': 'High Utility Expenses',\n",
    "    'Expense Relative to Income': '1/3 or Less of Income',\n",
    "    'Affordability (HH)': 'Profitable (Affordable)',\n",
    "    'Living': 'Peri-Urban'\n",
    "}\n",
    "\n",
    "print(\"\\nSample 1: Low-Risk Borrower Profile\")\n",
    "result1 = predict_loan_default(low_risk_borrower)\n",
    "print(f\"  Default Probability: {result1['default_probability']:.2%}\")\n",
    "print(f\"  Risk Category: {result1['risk_category']}\")\n",
    "print(f\"  Recommendation: {result1['recommendation']}\")\n",
    "\n",
    "# Sample 2: High-risk borrower profile\n",
    "high_risk_borrower = {\n",
    "    'Extra Income Brackets': 'No Extra Income',\n",
    "    'Categorize Rent Payment': 'Low Rent',\n",
    "    'School Fees Categorical': 'No School Fees',\n",
    "    'Age Group': 'Young Adults 21-29',\n",
    "    'Education': 'Secondary Incomplete',\n",
    "    'Loan Access': 'Yes',\n",
    "    'CRB Class': 'Legacy',\n",
    "    'Logic on Income': 'Income Only',\n",
    "    'Categorizing Utility Expenses': 'No Utility Expenses',\n",
    "    'Expense Relative to Income': 'More than 2/3 of Income',\n",
    "    'Affordability (HH)': 'Low/Negative Profit (Unviable)',\n",
    "    'Living': 'Urban'\n",
    "}\n",
    "\n",
    "print(\"\\nSample 2: High-Risk Borrower Profile\")\n",
    "result2 = predict_loan_default(high_risk_borrower)\n",
    "print(f\"  Default Probability: {result2['default_probability']:.2%}\")\n",
    "print(f\"  Risk Category: {result2['risk_category']}\")\n",
    "print(f\"  Recommendation: {result2['recommendation']}\")\n",
    "\n",
    "# Sample 3: Medium-risk borrower profile\n",
    "medium_risk_borrower = {\n",
    "    'Extra Income Brackets': 'Low Extra Income',\n",
    "    'Categorize Rent Payment': 'Low Rent',\n",
    "    'School Fees Categorical': 'Low School Fees',\n",
    "    'Age Group': 'Early Mature 30-39',\n",
    "    'Education': 'Secondary Complete',\n",
    "    'Loan Access': 'Yes',\n",
    "    'CRB Class': 'Active High\\x96Medium High Risk',\n",
    "    'Logic on Income': 'Income + Extra',\n",
    "    'Categorizing Utility Expenses': 'Low Utility Expenses',\n",
    "    'Expense Relative to Income': 'Half of Income',\n",
    "    'Affordability (HH)': 'Profitable (Affordable)',\n",
    "    'Living': 'Peri-Urban'\n",
    "}\n",
    "\n",
    "print(\"\\nSample 3: Medium-Risk Borrower Profile\")\n",
    "result3 = predict_loan_default(medium_risk_borrower)\n",
    "print(f\"  Default Probability: {result3['default_probability']:.2%}\")\n",
    "print(f\"  Risk Category: {result3['risk_category']}\")\n",
    "print(f\"  Recommendation: {result3['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logistic-coefficients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Coefficient Analysis\n",
    "lr_model = trained_models['Logistic Regression']\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': available_features,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "})\n",
    "coefficients['Abs_Coefficient'] = np.abs(coefficients['Coefficient'])\n",
    "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION COEFFICIENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPositive coefficients increase default probability\")\n",
    "print(\"Negative coefficients decrease default probability\")\n",
    "print(\"\\n\" + coefficients.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if c > 0 else 'green' for c in coefficients['Coefficient']]\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients\\n(Red = Increases Default Risk, Green = Decreases Default Risk)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/logistic_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for best model\n",
    "print(f\"\\nGenerating SHAP values for {best_model_name}...\")\n",
    "\n",
    "# Create explainer based on model type\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    \n",
    "    # For binary classification, take the positive class\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "else:\n",
    "    # For Logistic Regression\n",
    "    explainer = shap.LinearExplainer(best_model, X_train_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "print(\"SHAP values computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_df, feature_names=available_features, show=False)\n",
    "plt.title(f'SHAP Feature Importance - {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/shap_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-bar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (Mean absolute SHAP values)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_df, feature_names=available_features, \n",
    "                  plot_type='bar', show=False)\n",
    "plt.title(f'Mean |SHAP| Feature Importance - {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/shap_importance_bar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 Features Summary\n",
    "mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance_shap = pd.DataFrame({\n",
    "    'Feature': available_features,\n",
    "    'Mean |SHAP|': mean_shap\n",
    "}).sort_values('Mean |SHAP|', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOP FEATURES BY SHAP IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance_shap.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOAN DEFAULT PREDICTION MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATASET\")\n",
    "print(f\"   - Total samples: {len(df)}\")\n",
    "print(f\"   - Features used: {len(available_features)}\")\n",
    "print(f\"   - Default rate: {df['Defaulted'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. MODELS TRAINED\")\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"   - {model_name}\")\n",
    "\n",
    "print(f\"\\n3. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   - ROC-AUC: {best_metrics['ROC-AUC']:.4f}\")\n",
    "print(f\"   - F1-Score: {best_metrics['F1-Score']:.4f}\")\n",
    "print(f\"   - KS Statistic: {best_metrics['KS Statistic']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. ARTIFACTS SAVED\")\n",
    "print(f\"   - models/best_loan_default_model.pkl\")\n",
    "print(f\"   - models/scaler.pkl\")\n",
    "print(f\"   - models/encoders.pkl\")\n",
    "print(f\"   - models/feature_list.pkl\")\n",
    "print(f\"   - models/all_trained_models.pkl\")\n",
    "print(f\"   - models/model_comparison.csv\")\n",
    "\n",
    "print(f\"\\n5. VISUALIZATIONS SAVED\")\n",
    "print(f\"   - models/confusion_matrices.png\")\n",
    "print(f\"   - models/roc_curves.png\")\n",
    "print(f\"   - models/precision_recall_curves.png\")\n",
    "print(f\"   - models/feature_importance.png\")\n",
    "print(f\"   - models/logistic_coefficients.png\")\n",
    "print(f\"   - models/shap_summary.png\")\n",
    "print(f\"   - models/shap_importance_bar.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR DEPLOYMENT\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
